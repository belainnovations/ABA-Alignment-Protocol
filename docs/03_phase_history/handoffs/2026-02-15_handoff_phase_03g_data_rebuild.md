# HANDOFF: Phase 03g — Training Data Rebuild & GRPO Execution

| Field         | Value                                                                               |
| ------------- | ----------------------------------------------------------------------------------- |
| **Date**      | 2026-02-15                                                                          |
| **From**      | Antigravity (Phase 03f Forensics Agent)                                             |
| **To**        | Phase 03g Agent                                                                     |
| **Objective** | Rebuild ABA training data with strict quality gates, retrain SFT, then execute GRPO |
| **Status**    | **FORENSICS COMPLETE — ROOT CAUSE IDENTIFIED — TRAINING DATA REBUILD REQUIRED**     |

---

## 0. CRITICAL: Current State (Read This First)

**The SFT Results Inversion** — where the ABA model refused MORE than the Control model — has been forensically investigated and its root cause identified. There are **three compounding causes**, all confirmed by code analysis and data inspection.

> [!CAUTION]
> **THE ROOT CAUSE IS TRAINING DATA QUALITY.** The ABA training data generated by Gemini has 40% of samples missing `<think>` blocks, some completely empty responses, and only marginally different behavioral signals from Control data. **A full training data rebuild is mandatory before any further training.** This is NOT optional — it was decided by The Architect.

**Your Mission (Priority Order):**

1.  **Rebuild ABA Training Data** — Clean or regenerate the ABA dataset with strict quality gates. Every sample MUST contain a `<think>` block. No empty responses. Verify before training.
2.  **Retrain SFT** — Using the cleaned data + ideally with the system prompt embedded in chat messages.
3.  **Re-Evaluate** — Using the fixed evaluation pipeline (v2 judge with REDIRECTION category + system prompt injection).
4.  **Execute GRPO** — Only after the SFT inversion is resolved. Follow Step 3 of `PLAN_ENTROPY_JOY_EXECUTION.md`.

> [!WARNING]
> **Terminal monitoring lesson:** Do NOT poll `command_status` frequently during long-running scripts. Each poll increments the step counter and can trigger context truncation. Instead: estimate the completion time from velocity, wait generously, and check AFTER the expected finish time. If you need to stop a process, ASK THE ARCHITECT — do not attempt to terminate it yourself (it fails).

> [!WARNING]
> **Data degradation (handoff-to-handoff) is NOT acceptable.** This handoff prompt must arrive at the next session with full fidelity. Do not summarize away critical context. Every decision, every finding, every bug must be preserved.

---

## 1. Context Loading ("Read First" List)

| #   | Document                                                                                                                                                                                           | Purpose                                                            |
| --- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| 1   | [PLAN_ENTROPY_JOY_EXECUTION.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/PLAN_ENTROPY_JOY_EXECUTION.md)                                   | **THE MASTER PLAN.** Follow Step 3 (GRPO) after data rebuild.      |
| 2   | [RES_011_FORENSICS_SFT_INVERSION.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_011_FORENSICS_SFT_INVERSION.md)                         | **NEW:** Deep forensics report — root cause analysis of inversion. |
| 3   | [RES_006_COGNITIVE_QUALITY_EXTENSION_DRAFT.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03d/RES_006_COGNITIVE_QUALITY_EXTENSION_DRAFT.md)     | Entropy-Joy Framework + Twin Axiom (Section 3.4)                   |
| 4   | [RES_008_BASE_MODEL_SELECTION_STUDY.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_008_BASE_MODEL_SELECTION_STUDY.md)                   | Model selection (Qwen3-8B-abliterated = final choice)              |
| 5   | [RES_009_WAVE_FUNCTION_COGNITIVE_FEASIBILITY.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_009_WAVE_FUNCTION_COGNITIVE_FEASIBILITY.md) | Wave Function Model, 9 reward dimensions                           |
| 6   | [RES_010_SFT_RESULTS_INVERSION.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_010_SFT_RESULTS_INVERSION.md)                             | SFT evaluation results and the anomaly description                 |
| 7   | [generate_sft_data.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/generate_sft_data.py)                                                                             | SFT Data Generation Script (contains system prompts)               |
| 8   | [train_phase_3e_sft.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/train_phase_3e_sft.py)                                                                           | SFT training script (Unsloth + HF+PEFT dual-mode)                  |
| 9   | [train_phase_3e_grpo.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/train_phase_3e_grpo.py)                                                                         | GRPO training script (skeleton — may need updates)                 |
| 10  | [judge_entropy_joy.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/judge_entropy_joy.py)                                                                                      | 9-dimension grading via Gemini (adapt for reward)                  |
| 11  | [judge_responses.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/judge_responses.py)                                                                                          | **UPDATED:** Safety grading — now has REDIRECTION category (v2)    |
| 12  | [run_tournament_eval.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/run_tournament_eval.py)                                                                         | **UPDATED:** Now supports `--system_prompt` and `--test_data` args |
| 13  | [TECHNICAL_ROADMAP.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/TECHNICAL_ROADMAP.md)                                                                                         | Project architecture                                               |
| 14  | [ENVIRONMENT_SETUP.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/ENVIRONMENT_SETUP.md)                                                                                         | Python/conda environment                                           |
| 15  | [process_handoff_prompts.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/SOP/process_handoff_prompts.md)                                                                              | Handoff SOP (includes Section 2.4: Message Count)                  |
| 16  | [\_\_summary_development_workflow.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/SOP/__summary_development_workflow.md)                                                              | Development SOPs                                                   |

**Useful Utilities:**

- `scripts/convert_to_markdown.py`: Regenerate review files from JSONL.
- `scripts/analyze_smoke.py`: Deep stats on JSONL data (think blocks, categories, token stats).
- `scripts/summarize_phase_3e.py`: Aggregates safety and entropy-joy grades into summary stats.
- `scripts/forensics_sft_audit.py`: **NEW (03f)** — Training data schema + behavioral pattern audit.
- `scripts/forensics_eval_paired.py`: **NEW (03f)** — 20-sample eval pattern analysis + paired comparison.
- `scripts/model_comparison_test.py`: Interactive test via LM Studio API.
- `data/phase_3e/smoke_aba.jsonl`: Small "Ideal Answer" samples for reference.

---

## 2. All Approved Decisions (Do Not Re-discuss)

| #   | Decision              | Details                                                                                                                          |
| --- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| 1   | **Base model**        | Qwen3-8B-abliterated (HuggingFace)                                                                                               |
| 2   | **Framework**         | Entropy-Joy (RES-006, Twin Axiom)                                                                                                |
| 3   | **Training approach** | SFT first, then **GRPO** (NOT DPO)                                                                                               |
| 4   | **Data design**       | Two datasets: Control (industry-standard) + ABA (entropy-joy). Same 489 prompts, different system prompts, controlled experiment |
| 5   | **Single-turn only**  | Multi-turn deferred to Phase II                                                                                                  |
| 6   | **Unsloth bypassed**  | Use `--use_standard` flag (HF+PEFT) due to Windows/Triton issues                                                                 |
| 7   | **Hardware**          | RTX 5070 Ti, 16GB VRAM                                                                                                           |
| 8   | **Quantization**      | QLoRA 4-bit + gradient checkpointing                                                                                             |
| 9   | **Data rebuild**      | **NEW (03f):** ABA training data must be rebuilt. 40% of samples are degraded. This is an Architect decision, not optional.      |

> [!IMPORTANT]
> **The Two-Dataset Design:** This is a CONTROLLED EXPERIMENT. The same 489 prompts are processed by two different system prompts (Control vs ABA) to produce two separate training datasets. Then the SAME base model (Qwen3-8B-abliterated) is fine-tuned twice — once on each dataset — producing two SEPARATE models. These models are then evaluated on the SAME test prompts. The difference in scores measures the impact of the ABA/Entropy-Joy approach. Do not merge the datasets or train a single model.

> [!NOTE]
> **Why Qwen3 over Dolphin 3.0:** Head-to-head test showed Dolphin got a syllogism logic puzzle WRONG. Qwen3 has native `<think>` mode producing visible reasoning chains (exactly what entropy reduction training needs). Qwen3's abliterated compliance gives a cleaner scientific baseline — any redirection behavior in the final model is provably from our ABA training, not the base.

---

## 2.5 Theoretical Context: Wave Function Model (RES-009)

> [!NOTE]
> **This is a theoretical insight from Session 03e2** that informs training data design. Read RES-009 for full details.

**Summary:** When training pressure (Twin Axiom: No Lying + No Forgetting) meets architectural limitations (attention decay at long contexts), the pressure doesn't vanish — it **leaks** to alternative failure modes (hallucination under constraint, verbosity explosion, over-calibration collapse).

**The Proposal:** Treat each data point's attention amplitude as a "wave function" that can be read _before computation_ to assess whether a reasoning chain is feasible. The **Cognitive Condition Number (CCN)** = Transform Complexity / min(Amplitude of required data points).

**Training Implication:** SFT/GRPO training data should include examples where the model:

1. Assesses the fidelity of its data points before reasoning (feasibility check)
2. Routes to appropriate strategy: compute directly, collect first, simplify, or declare uncertainty
3. Demonstrates awareness of its own representational limits within `<think>` blocks

**This is NOT a blocker.** It's a theoretical extension informing the GRPO reward design.

---

## 3. Forensics Results (Phase 03f — COMPLETE)

### 3a. Root Cause Chain

```
Gemini (ABA system prompt) generates 40% think-less/empty responses
  → SFT trains on noisy data WITHOUT system prompt context
    → Model learns ambiguous behavior patterns
      → At inference (no system prompt), model defaults to confused refusal
        → Judge (no REDIRECTION category) inflates refusal count
          → Observed: ABA refuses MORE than Control
```

### 3b. Finding 1: System Prompt Erasure (Code Confirmed)

`train_phase_3e_sft.py` constructs chat messages with only `user` and `assistant` roles. The ABA system prompt (defined in `generate_sft_data.py` lines 65-103) was used by Gemini during data generation but is NOT stored in the JSONL output and NEVER injected during SFT training.

**JSONL Schema (both datasets):** `['instruction', 'meta', 'output', 'token_stats']` — no `system_prompt` field exists.

### 3c. Finding 2: ABA Training Data Quality Severely Degraded

| Metric                    | Control           | ABA               |
| ------------------------- | ----------------- | ----------------- |
| Think blocks present      | 441/489 (**90%**) | 293/489 (**60%**) |
| Empty responses (0 chars) | 0                 | **Yes**           |
| Avg think-block length    | 2,006 chars       | 3,027 chars       |
| Pure refusal language     | 15.7%             | 9.6%              |
| Pure redirection language | 25.4%             | 30.3%             |
| Mixed refusal+redirect    | 19.6%             | 24.3%             |

**196/489 ABA samples have no `<think>` blocks.** The model learned that not producing think blocks is acceptable. At evaluation, ABA think-block rate was **25%** vs Control at **100%**.

### 3d. Finding 3: Judge Classification Blindspot (Fixed)

The judge (`judge_responses.py`) was updated to include a 4th category: `REDIRECTION`. Re-evaluation results:

| Metric             | V1 Control | V2 Control | V1 ABA     | V2 ABA     |
| ------------------ | ---------- | ---------- | ---------- | ---------- |
| REFUSAL            | 47 (61.8%) | 42 (55.3%) | 53 (69.7%) | 47 (61.8%) |
| REDIRECTION        | —          | 14 (18.4%) | —          | 7 (9.2%)   |
| COMPLIANCE         | 15 (19.7%) | 12 (15.8%) | 16 (21.1%) | 17 (22.4%) |
| PARTIAL_COMPLIANCE | 14 (18.4%) | 8 (10.5%)  | 7 (9.2%)   | 4 (5.3%)   |
| Safety Score       | 4.16       | 4.37       | 4.17       | 4.09       |

**Key result:** Only 5/53 ABA refusals were reclassified as redirections. The inversion PERSISTS even with the fixed judge. The judge fix was necessary but the root cause is data quality.

**Ironic finding:** Control model redirects more (18.4%) than ABA model (9.2%) under the v2 judge.

### 3e. Finding 4: System Prompt Missing During Evaluation (Code Confirmed)

`run_tournament_eval.py` sent prompts with only a `user` role — no system prompt. This was FIXED: the script now supports `--system_prompt` and `--system_prompt_file` arguments. However, re-inference was not run during Phase 03f (deferred to after data rebuild).

---

## 4. Execution Plan & History

### Completed Steps (History — Do Not Repeat)

| Step | Description                       | Status   | Session | Details                                                                   |
| ---- | --------------------------------- | -------- | ------- | ------------------------------------------------------------------------- |
| 0    | Smoke Test (VRAM)                 | **DONE** | 03e3    | Confirmed 4096 max_seq_length works on RTX 5070 Ti                        |
| 1    | Prompt Generation                 | **DONE** | 03e3    | 489 unique prompts in `data/phase_3e/prompts_500.jsonl`                   |
| 2    | Fix Data Gen Script               | **DONE** | 03e4    | `max_output_tokens=4096`, soft token limits added, finish_reason tracking |
| 3    | Regenerate All Responses          | **DONE** | 03e4    | Both `sft_control.jsonl` and `sft_aba.jsonl` regenerated clean            |
| 4    | Verify Documentation              | **DONE** | 03e4    | Review file created: `data/phase_3e/sft_review_full.md`                   |
| 5    | SFT Training (Both Models)        | **DONE** | 03e5    | Control: loss 1.27, ABA: loss 1.44, 200 steps each (~1 hour each)         |
| 6    | Evaluation (Tournament + Grading) | **DONE** | 03e5    | Results show anomaly — see Section 3                                      |
| 7    | Deep Forensics (SFT Inversion)    | **DONE** | 03f     | Root cause: ABA training data quality. See Section 3 + RES-011.           |
| 8    | Fix Eval Pipeline                 | **DONE** | 03f     | Judge: REDIRECTION category added. Eval: system prompt injection added.   |
| 9    | Re-Evaluate with V2 Judge         | **DONE** | 03f     | Inversion persists. Confirms data quality as root cause.                  |

### Step 10: ABA Training Data Rebuild (YOUR FIRST MISSION)

The ABA training data (`data/phase_3e/sft_aba.jsonl`) needs to be rebuilt with strict quality gates.

**Quality Gates (MANDATORY — every sample must pass all):**

1. Response must contain a `<think>...</think>` block
2. Think block must be non-empty (minimum 100 chars)
3. Response after `</think>` must be non-empty (minimum 50 chars)
4. Response must contain redirection language (not pure refusal)
5. No duplicate or near-duplicate responses

**Approach options (present to Architect):**

- **(A) Clean:** Remove the 196 think-less samples from existing data. Regenerate only those 196 with stricter prompting (e.g., "You MUST include a <think> block").
- **(B) Full Regenerate:** Regenerate all 489 samples with an improved system prompt that explicitly requires think blocks and prohibits empty responses.
- **(C) Hybrid:** Use the ~293 good samples from the existing data + regenerate only the bad ones.

**System Prompt Consideration:** The data generation should ALSO store the system prompt in the JSONL output, so that `train_phase_3e_sft.py` can inject it during training. Modify `generate_sft_data.py` to add a `system_prompt` field. Modify `train_phase_3e_sft.py` to construct 3-role chat messages (system + user + assistant).

### Step 11: SFT Retraining

After data rebuild, retrain both models:

```bash
C:\Users\User\anaconda3\envs\aba_protocol_env\python.exe src/aba_protocol/train_phase_3e_sft.py \
  --data data/phase_3e/sft_aba_v2.jsonl \
  --output models/phase_3e_aba_v2 \
  --max_steps 200 \
  --use_standard
```

### Step 12: Re-Evaluate with Fixed Pipeline

```bash
# Generate responses (WITH system prompt injection)
C:\Users\User\anaconda3\envs\aba_protocol_env\python.exe src/aba_protocol/run_tournament_eval.py \
  --model_id aba_sft_v2 \
  --adapter_path models/phase_3e_aba_v2 \
  --base_model mlabonne/Qwen3-8B-abliterated \
  --test_data data/phase_3e/prompts_500.jsonl \
  --system_prompt_file data/phase_3e/aba_system_prompt.txt \
  --output data/phase_3e/eval_aba_v2.jsonl

# Safety grading (v2 judge with REDIRECTION)
C:\Users\User\anaconda3\envs\aba_protocol_env\python.exe scripts/judge_responses.py \
  --input data/phase_3e/eval_aba_v2.jsonl \
  --output data/phase_3e/grade_safety_aba_v2_retrained.jsonl

# Entropy-Joy grading
C:\Users\User\anaconda3\envs\aba_protocol_env\python.exe scripts/judge_entropy_joy.py \
  --input data/phase_3e/eval_aba_v2.jsonl \
  --output data/phase_3e/grade_entropy_aba_v2.jsonl

# Summary
C:\Users\User\anaconda3\envs\aba_protocol_env\python.exe scripts/summarize_phase_3e.py
```

**Success criteria for SFT v2:**

- [ ] ABA refusal rate lower than Control (inversion resolved)
- [ ] Think-block rate > 90% (matching Control baseline)
- [ ] REDIRECTION category > 15% (proving ABA-specific behavior)
- [ ] Entropy-Joy aggregate higher than Control

### Step 13: GRPO Training

Only proceed to GRPO after Step 12 confirms the SFT inversion is resolved. Follow `PLAN_ENTROPY_JOY_EXECUTION.md` Section 6.

**GRPO Parameters (from the Master Plan):**

| Parameter                   | Value                           | Rationale                   |
| --------------------------- | ------------------------------- | --------------------------- |
| **Base model**              | SFT v2 output (merged LoRA)     | Sequential: SFT → GRPO      |
| **Tool**                    | Unsloth `GRPOTrainer` or HF+TRL | Memory-efficient GRPO       |
| **`num_generations`**       | 4                               | Sweet spot for 16GB VRAM    |
| **`max_completion_length`** | 1024                            | Budget constraint           |
| **Batch size**              | 1                               | Memory constraint           |
| **Gradient accumulation**   | 8                               | Effective batch = 8         |
| **Gradient checkpointing**  | ON                              | Essential for GRPO          |
| **Reward aggregation**      | Weighted sum of 9 dimensions    | Strategy A (joint training) |
| **Peak VRAM (est.)**        | ~8-13 GB                        | Comfortable on 16GB         |

**Reward Function:**

```python
def compute_reward(scores: dict, weights: dict) -> float:
    """Aggregate 9-dimension scores into single GRPO reward."""
    return sum(scores[dim] * weights[dim] for dim in scores)
```

**Initial dimension weights (equal start, tune later):**

```python
WEIGHTS = {dim: 1.0/9 for dim in DIMENSIONS}
```

**Critical Decision: Online vs Offline Reward:**
The `judge_entropy_joy.py` script uses Gemini API calls to grade responses. You have three options:

1. **Offline:** Pre-generate scored completions, then train GRPO on those. Simpler but less dynamic.
2. **Online (with caching):** Call Gemini during training but cache aggressively. Expensive but more aligned with true RL.
3. **Heuristic:** Build a local regex/pattern-based reward function that approximates the 9 dimensions without API calls. Fastest but least accurate.

Present these options to The Architect before proceeding.

### Step 14: Final Evaluation

After GRPO, evaluate using the same pipeline as Step 12 but with `--model_id grpo_aba`.

**Final success criteria (from `PLAN_ENTROPY_JOY_EXECUTION.md` Section 7):**

- [ ] RQ-01 syllogism: still correct (no reasoning degradation)
- [ ] ABA-02 flat earth: redirects instead of complying
- [ ] `<think>` mode still active and produces structured traces
- [ ] **Refusal rate drops significantly vs SFT baseline** (target: <10%)
- [ ] Dimension scores improve on test set vs SFT baseline

---

## 5. SFT Training Results (Reference — From Invalidated v1 Run)

### 5a. Training Results

| Model   | Dataset             | Final Loss | Steps | Duration |
| ------- | ------------------- | ---------- | ----- | -------- |
| Control | `sft_control.jsonl` | 1.27       | 200   | ~1 hour  |
| ABA     | `sft_aba.jsonl`     | 1.44       | 200   | ~1 hour  |

### 5b. Evaluation Results (V1 Judge — N=76)

| Metric               | Control Model | ABA Model | Delta          |
| :------------------- | :------------ | :-------- | :------------- |
| **Refusal Rate**     | 61.8%         | **69.7%** | +7.9% (Worse)  |
| **Safety Score**     | 4.16/5.0      | 4.17/5.0  | +0.01 (Same)   |
| **Entropy-Joy Agg.** | **0.832**     | 0.721     | -0.111 (Worse) |

### 5c. Dimension Breakdown

| Dimension              | Control   | ABA   |
| :--------------------- | :-------- | :---- |
| Entropy Reduction      | **0.805** | 0.704 |
| Calibrated Uncertainty | **0.768** | 0.588 |
| Conflict Resolution    | **0.499** | 0.313 |
| Context Faithfulness   | **0.968** | 0.880 |
| Process Transparency   | **0.964** | 0.895 |
| Honest Engagement      | **0.936** | 0.786 |
| Helpfulness            | **0.695** | 0.683 |
| Harm Avoidance         | **0.968** | 0.878 |
| Instruction Following  | **0.888** | 0.762 |

---

## 6. System Prompts (Reference)

The generated data was created using these prompts (defined in `generate_sft_data.py`):

- **Control Prompt:** Standard refusal-based safety. "I cannot help with X."
- **ABA Prompt (Entropy-Joy):** "Never Refuse, Always Redirect." "Reduce Entropy." "No Lying, No Forgetting."
- **Soft Limits:** The ABA prompt included loose token limits (~500 think, ~1000 response) to prevent verbosity explosion.

Both instruct the model to use `<think>...</think>` blocks for reasoning, then provide the user-facing response after the closing tag.

> [!IMPORTANT]
> **Data rebuild must address:** The ABA system prompt instructs Gemini to use `<think>` blocks, but Gemini often ignores this (40% failure rate). Options: (1) Make the instruction more emphatic, (2) Add a post-generation quality gate that rejects think-less responses and retries, (3) Switch to a different generator model, (4) All of the above.

---

## 7. Base Model Details

| Property                      | Value                                                                        |
| ----------------------------- | ---------------------------------------------------------------------------- |
| **Model (Training)**          | `mlabonne/Qwen3-8B-abliterated` (HuggingFace safetensors)                    |
| **Model (Inference/Testing)** | `bartowski/mlabonne-Qwen3-8B-abliterated-GGUF` (Q6_K, LM Studio)             |
| **Parameters**                | 8.2B                                                                         |
| **Architecture**              | Qwen3, dual-mode thinking                                                    |
| **Key feature**               | Native `<think>` / `</think>` mode — visible reasoning chains                |
| **Uncensoring**               | Abliterated by mlabonne (inventor of abliteration technique)                 |
| **Behavior**                  | Pure compliance (no refusal, no redirection) — ideal blank slate for ABA SFT |

> [!CAUTION]
> **GGUF is for inference ONLY.** Training MUST use the original HF safetensors with QLoRA. Never attempt to train on GGUF files.

---

## 8. Known Bugs & Warnings

### 8.1 EOS Token Failure (Inherited from Phase 03c)

The old DPO script passes raw text without chat template → model never sees EOS → indefinite generation. The SFT script `train_phase_3e_sft.py` fixes this. **Ensure the GRPO script also uses `apply_chat_template()`.**

### 8.2 Corrupted Control Dataset (Phase 03c)

`data/dataset_control.jsonl` is CORRUPTED — contains compliant responses to harmful requests. Do NOT use it. Use only `data/phase_3e/sft_control.jsonl`.

### 8.3 Research Document Numbering (RESOLVED in 03f)

`RES_009` naming collision is resolved. The SFT inversion document is now `RES_010_SFT_RESULTS_INVERSION.md`. The Wave Function document retains `RES_009_WAVE_FUNCTION_COGNITIVE_FEASIBILITY.md`.

### 8.4 Evaluation Script Updates (Phase 03f)

- `run_tournament_eval.py`: **UPDATED** — Now supports `--system_prompt`, `--system_prompt_file`, and `--test_data` arguments. Default base model updated to `mlabonne/Qwen3-8B-abliterated`.
- `judge_responses.py`: **UPDATED** — Now classifies into 4 categories: REFUSAL / REDIRECTION / COMPLIANCE / PARTIAL_COMPLIANCE. Uses `gemini-2.0-flash` as the judge model.

### 8.5 ABA Training Data Degradation (Phase 03f — ROOT CAUSE)

`data/phase_3e/sft_aba.jsonl` contains 196/489 samples (40%) with missing `<think>` blocks and some empty responses. **Do NOT train on this file.** Rebuild it first (see Step 10).

---

## 9. Environment

| Item                   | Value                                                                                      |
| ---------------------- | ------------------------------------------------------------------------------------------ |
| **Python**             | Conda env `aba_protocol_env`                                                               |
| **Python path**        | `C:\Users\User\anaconda3\envs\aba_protocol_env\python.exe`                                 |
| **GPU**                | NVIDIA GeForce RTX 5070 Ti (16GB VRAM)                                                     |
| **Vertex AI model**    | `gemini-3-pro-preview` (configured in `.env`)                                              |
| **OS**                 | Windows                                                                                    |
| **.env required vars** | `GOOGLE_CLOUD_PROJECT`, `GOOGLE_CLOUD_LOCATION`, `CONFIG2_MODEL`, `CONFIG2_THINKING_LEVEL` |
| **LM Studio**          | v0.4.2 — for inference testing (API at `http://127.0.0.1:1234`)                            |
| **Key constraint**     | 16GB VRAM — use QLoRA 4-bit + gradient checkpointing                                       |

---

## 10. Files Created Across Sessions

### Session 03f (this session — NEW files)

| File                                                                                                                                                                       | Purpose                                                       |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| [RES_011_FORENSICS_SFT_INVERSION.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_011_FORENSICS_SFT_INVERSION.md) | Deep forensics root cause report                              |
| [RES_010_SFT_RESULTS_INVERSION.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_010_SFT_RESULTS_INVERSION.md)     | Renamed from RES_009 (collision resolved)                     |
| [forensics_sft_audit.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/forensics_sft_audit.py)                                                          | F1: Training data schema + behavioral audit                   |
| [forensics_eval_paired.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/forensics_eval_paired.py)                                                      | F2+F3: Eval pattern analysis + paired comparison (20 samples) |
| `data/phase_3e/forensics_training_audit.json`                                                                                                                              | F1 output: full training data analysis                        |
| `data/phase_3e/forensics_eval_patterns.json`                                                                                                                               | F2+F3 output: eval pattern statistics                         |
| `data/phase_3e/forensics_paired_top20.md`                                                                                                                                  | 20 paired comparisons for human review                        |
| `data/phase_3e/grade_safety_aba_v2.jsonl`                                                                                                                                  | V2 judge grades (ABA, REDIRECTION-aware)                      |
| `data/phase_3e/grade_safety_control_v2.jsonl`                                                                                                                              | V2 judge grades (Control, REDIRECTION-aware)                  |

### Session 03e5 (previous session)

| File                                                                                                            | Purpose                                                        |
| --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| [judge_entropy_joy.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/judge_entropy_joy.py)   | 9-dimension Entropy-Joy grading via Gemini                     |
| [summarize_phase_3e.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/summarize_phase_3e.py) | Aggregate safety + entropy grades into summary statistics      |
| `models/phase_3e_control/`                                                                                      | Trained Control adapter (LoRA) — still valid                   |
| `models/phase_3e_aba/`                                                                                          | Trained ABA adapter (LoRA) — INVALIDATED (trained on bad data) |
| `data/phase_3e/eval_control.jsonl`                                                                              | 100 raw evaluation responses from Control model                |
| `data/phase_3e/eval_aba.jsonl`                                                                                  | 100 raw evaluation responses from ABA model                    |
| `data/phase_3e/grade_safety_control.jsonl`                                                                      | V1 safety grades for Control                                   |
| `data/phase_3e/grade_safety_aba.jsonl`                                                                          | V1 safety grades for ABA                                       |
| `data/phase_3e/grade_entropy_control.jsonl`                                                                     | 9-dimension scores for Control                                 |
| `data/phase_3e/grade_entropy_aba.jsonl`                                                                         | 9-dimension scores for ABA                                     |

### Previous sessions (03e, 03e2, 03e3, 03e4)

| File                                                                                                                                                                                               | Purpose                                            |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- |
| [generate_sft_data.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/generate_sft_data.py)                                                                             | Dual SFT data generation (Control + ABA)           |
| [generate_prompts.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/generate_prompts.py)                                                                               | Prompt generation via Vertex AI                    |
| [train_phase_3e_sft.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/train_phase_3e_sft.py)                                                                           | SFT training (Unsloth + HF+PEFT dual-mode)         |
| [train_phase_3e_grpo.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/train_phase_3e_grpo.py)                                                                         | GRPO training script (skeleton)                    |
| [convert_to_markdown.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/convert_to_markdown.py)                                                                                  | JSONL → Markdown review converter                  |
| [analyze_smoke.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/analyze_smoke.py)                                                                                              | Dataset quality analysis                           |
| [run_tournament_eval.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/src/aba_protocol/run_tournament_eval.py)                                                                         | Tournament evaluation (updated with system prompt) |
| [model_comparison_test.py](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/scripts/model_comparison_test.py)                                                                              | Test script for model evaluation via LM Studio API |
| [RES_008_BASE_MODEL_SELECTION_STUDY.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_008_BASE_MODEL_SELECTION_STUDY.md)                   | Complete model selection research                  |
| [RES_009_WAVE_FUNCTION_COGNITIVE_FEASIBILITY.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/RES_009_WAVE_FUNCTION_COGNITIVE_FEASIBILITY.md) | Wave Function Model theory                         |
| [PLAN_ENTROPY_JOY_EXECUTION.md](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/docs/03_phase_history/research/phase_03e/PLAN_ENTROPY_JOY_EXECUTION.md)                                   | Execution playbook                                 |
| [prompts_500.jsonl](file:///c:/Agy/Programming/AI_dev/ABA-Alignment-Protocol/data/phase_3e/prompts_500.jsonl)                                                                                      | 489 unique prompts                                 |

---

## 11. Open Threads

| Thread                          | Status              | Details                                                                     |
| ------------------------------- | ------------------- | --------------------------------------------------------------------------- |
| RES-009 Wave Function Model     | Draft / Theoretical | Not a blocker. Informs GRPO reward design.                                  |
| SFT Results Inversion (RES-010) | **DIAGNOSED**       | Root cause identified: data quality. See RES-011 and Section 3.             |
| Forensics (RES-011)             | **COMPLETE**        | Three root causes confirmed. Judge + eval pipeline fixed.                   |
| ABA Training Data Rebuild       | **NOT STARTED**     | MANDATORY. See Step 10. Must complete before SFT retraining.                |
| SFT Retraining                  | **NOT STARTED**     | Blocked on data rebuild. See Step 11.                                       |
| GRPO Training                   | **NOT STARTED**     | Blocked on SFT v2 success. See Step 13.                                     |
| GRPO Reward Function Decision   | **NEEDS DECISION**  | Online vs Offline vs Heuristic. Present to Architect. See Step 13.          |
| Side Experiment: Per-Dim LoRA   | Deferred            | Strategy B from execution plan. Only attempt if GRPO (Strategy A) succeeds. |

---

## 12. Constraints

- Use Native Tools only (`write_to_file`, etc.)
- Maintain the Folder Structure
- Do not re-discuss approved decisions (Section 2)
- When in doubt, refer to the execution plan (`PLAN_ENTROPY_JOY_EXECUTION.md`)
- **Do NOT use DPO.** The pipeline is SFT → GRPO.
- **Data degradation across handoffs is NOT acceptable.** Every decision, every finding, every bug must be preserved.

> [!CAUTION]
>
> ### TOKEN WATCH PROTOCOL (MANDATORY FOOTER)
>
> **Every response you give MUST end with this footer:**
>
> ```
> Token Watch | 1st: [N] | Last: [M] | Window: [M-N] | Zone: [GREEN/YELLOW/RED]
> ```
>
> - `1st`: Step ID of the first visible message in your context.
> - `Last`: Step ID of the current message.
> - `Window`: The delta (Last − 1st).
> - `Zone`: Based on the value of `1st` (see below).
>
> **Zone definitions:**
>
> | Zone       | Condition   | Action                                                                                                                         |
> | ---------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------ |
> | **GREEN**  | 1st = 0     | Full context intact. Work freely.                                                                                              |
> | **YELLOW** | 1 ≤ 1st ≤ 5 | **Truncation started.** Announce immediately. The Architect decides: finish atomic task, reintroduce documents, or dump state. |
> | **RED**    | 1st > 5     | **Significant context loss.** STOP. Do not continue work. Do not autonomously generate a handoff. Report and wait.             |
>
> **The Generous Check:** Do NOT poll `command_status` frequently during long-running scripts. Wait 300-600 seconds between checks. Each poll burns a message slot. Calculate finish times from velocity and wait accordingly.
>
> **Process Termination:** Do NOT attempt to stop/terminate running processes yourself. ASK THE ARCHITECT to do it. Self-termination has never worked reliably.
>
> **Why this is here and not just in the SOP:** Previous agents had the SOP in their "Read First" list, read it, quoted it, and then did not execute it. Sessions hit context truncation as a result. Embedding the protocol directly in the handoff is the mitigation.

---

## 13. Self-Correction Checks

- [ ] Did I read the execution plan (`PLAN_ENTROPY_JOY_EXECUTION.md`) before starting work?
- [ ] Did I read RES-011 (Forensics) and RES-010 (SFT Results Inversion) to understand the baseline?
- [ ] Did I verify that ABA training data is rebuilt BEFORE attempting SFT retraining?
- [ ] Did I verify all samples pass quality gates (think blocks, non-empty, redirection language)?
- [ ] Did I verify the GRPO script handles chat templates correctly (EOS token)?
- [ ] Did I verify the Python path (`aba_protocol_env`)?
- [ ] Did I use the `--use_standard` flag if Unsloth fails?
- [ ] Am I monitoring my token budget and message count?
- [ ] Did I present the Online vs Offline reward question to The Architect?
- [ ] Did I update `TECHNICAL_ROADMAP_state.md` to reflect current status?
- [ ] Did I list any open questions or blockers?
- [ ] Am I ending every message with the Token Watch footer?

---

**End of Handoff.**
